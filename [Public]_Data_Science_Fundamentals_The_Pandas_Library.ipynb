{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Public] Data Science Fundamentals: The Pandas Library",
      "provenance": [],
      "collapsed_sections": [
        "F7-uIOewCaLN",
        "4VSyLiB2jkXy",
        "hL8FF6AExTtt",
        "Bhy-1w_XxWsg",
        "tJTnOOwrJEht",
        "E-f4NsOhVdrp"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomlundquist/thinkful-git-ws/blob/master/%5BPublic%5D_Data_Science_Fundamentals_The_Pandas_Library.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQde2o3Ftx_v"
      },
      "source": [
        "# Data Science Fundamentals: The Pandas Library\n",
        "## Box Office Data\n",
        "\n",
        "Today, we will be exploring fundamental concepts of pandas data manipulation to prepare a data set for modeling. \n",
        "\n",
        "**Data set:** Information from IMDB about movies\n",
        "\n",
        "**Our Goal:** Process and clean the data to prepare it for modeling to predict the gross profit of a movie.\n",
        "\n",
        "You will need to add some code to complete this notebook.  Follow along with the instructor to find what code to add.  You will add that where the code says `***ADD CODE HERE***`\n",
        "\n",
        "Have fun and good luck coding!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7-uIOewCaLN"
      },
      "source": [
        "## Configuring Our Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VSyLiB2jkXy"
      },
      "source": [
        "### Importing Libraries\n",
        "\n",
        "One of the things that makes Python **great** for data science is all of the different libraries that exist so we don't have to code them from scratch. Tonight we'll be taking advantage of:\n",
        "- [Numpy](https://numpy.org/) for scientific and mathematical computing\n",
        "- [Pandas](https://pandas.pydata.org/) for data wrangling and analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XkkokDMOt221"
      },
      "source": [
        "# Data analysis packages\n",
        "***ADD CODE HERE***\n",
        "***ADD CODE HERE***\n",
        "\n",
        "# Default settings for pandas\n",
        "pd.set_option('mode.chained_assignment', None)\n",
        "pd.set_option('display.float_format', '{:,.2f}'.format)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL8FF6AExTtt"
      },
      "source": [
        "### Importing Data\n",
        "Pandas can work with information from all kinds of data sources. Below, we'll import the data we need from a GitHub URL and read it into a Pandas Dataframe using the Pandas [`read_csv`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html) function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQusb-oRuC8I"
      },
      "source": [
        "# Import data from github\n",
        "data = pd.***ADD CODE HERE***('https://github.com/autumntoney/predict_the_box_office/raw/master/movie_metadata.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bhy-1w_XxWsg"
      },
      "source": [
        "## Understand The Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UheSZMCouEc4"
      },
      "source": [
        "# Check out the first lines of the data set\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyJPSs4HEWnk"
      },
      "source": [
        "# Checking the size of our data (rows, columns)\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hm_MKY-RFKKw"
      },
      "source": [
        "# Get a concise summary of the dataset\n",
        "data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBhGCEOqE_sJ"
      },
      "source": [
        "# Understand the basic statistical details of the data set\n",
        "data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdMRriQNX7pd"
      },
      "source": [
        "__Missing Values:__ You can identify columns with missing values by looking at the `non-null` items in the `.info()` call, and the difference in number of data points in the `count` row of the `.describe()` function. For example, the `num_critic_for_reviews` column has a lower `count` than `duration`, so has missing values.\n",
        "\n",
        "__Outliers:__ You can also identify if there are outliers in the data set by looking at the `.describe()` function's spread of data in comparison to the `min` and `max` values.  For example, the `duration` column's `max` value is 511, which would correspond to an 8.5 hour film. You might want to go back and check that the values for that film are valid, and correct or drop the values that aren't.  \n",
        "\n",
        "NB: We don't have time to investigate outliers in this workshop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mI9J_ZlDHezH"
      },
      "source": [
        "## Clean The Data\n",
        "\n",
        "Now that we understand the basics of what's in the data, we now need to clean the data before it's ready for modeling.  Things we'll cover:\n",
        "- Duplicate data\n",
        "- Missing data\n",
        "- Manipulating data\n",
        "\n",
        "NB: Most models can only process numerical data, so we will focus our cleaning on those columns."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJTnOOwrJEht"
      },
      "source": [
        "### Rename Columns\n",
        "\n",
        "We want to update the names of some of our columns so they reflect units for things like time and money. This will make our work easier to understand by others."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A8jknTusBVcY"
      },
      "source": [
        "# Rename columns to include units\n",
        "data.***ADD CODE HERE***(columns={'duration': 'duration_mins',\n",
        "                    'budget': 'budget_usd',\n",
        "                    'gross': 'gross_usd'}, \n",
        "            inplace=True)\n",
        "data.head(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-f4NsOhVdrp"
      },
      "source": [
        "### Removing duplicates\n",
        "Since our data includes that of movies, we will want to check for duplicated `movie_title`, but since there could be movie remakes in here as well, we should also check the `title_year` using the Pandas [`duplicated`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.duplicated.html) function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I17BbgjUHrBR"
      },
      "source": [
        "# Checking to see what movies are duplicates.  \n",
        "# Sorting by movie title to see duplicates\n",
        "data[\n",
        "    data.***ADD CODE HERE***(\n",
        "        subset=['movie_title', 'title_year'], \n",
        "        keep=False)\n",
        "    ].sort_values('movie_title').head()  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6vGBj75XyMw"
      },
      "source": [
        "We do have duplicated movies in our data.  We need to drop those duplicates to remove them from our data set using the Pandas [`drop_duplicates`](https://pandas.pydata.org/pandas-docs/version/0.17/generated/pandas.DataFrame.drop_duplicates.html) function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6Zhd1QKmVik"
      },
      "source": [
        "# Drop all duplicate movie titles that were released in the same year\n",
        "data = data.drop_duplicates(subset=['movie_title', 'title_year'], keep='first').copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnnQZqXrZDMy"
      },
      "source": [
        "### Missing values\n",
        "Now that we've cleaned out the duplicates, let's take a look at the missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "paqsU9MXGE54"
      },
      "source": [
        "# Show how many values are missing from each column\n",
        "data.***ADD CODE HERE***.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0A06bffezm_2"
      },
      "source": [
        "There are a good number of films missing the `aspect_ratio`, and using our knowledge of movies, it is unlikely that the aspect ratio is important in predicting the gross revenue of the film.  Let's drop the whole column using the Pandas [`drop`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.drop.html) function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq0B_sZlz6Uu"
      },
      "source": [
        "# Drop the aspect ratio column (axis = 1)\n",
        "data.***ADD CODE HERE***"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ayeh_2mLsp7T"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Okofvg1U4PZ"
      },
      "source": [
        "Now, since we're wanting to eventually predict the `gross` revenue of the movie, let's take a look at the missing values of `gross`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N66-7bWQO5KA"
      },
      "source": [
        "# What percent of the gross values are missing?\n",
        "print(len(data[data['gross_usd'].isna()])/len(data))\n",
        "# Or, more simply\n",
        "data['gross_usd'].isnull().mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pIcHoYEMC7GN"
      },
      "source": [
        "# show what movies are missing the gross values\n",
        "data[data['gross_usd'].isna()].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sv5laWwoatDr"
      },
      "source": [
        "Since we eventually want to predict the gross revenue of a movie, having a null value for gross revenue will not help us train our model, so we should drop them using the Pandas [`dropna`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html) function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXVxzxnYOl8q"
      },
      "source": [
        "# Drop all null values of gross\n",
        "# You might be interested in how the movie performed\n",
        "# so don't want to impute these values (skew the analysis)\n",
        "data.dropna(subset=['gross_usd'], how='all', inplace=True)\n",
        "# Check the size of the data after dropping null values\n",
        "data.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iK86bmq4c2vB"
      },
      "source": [
        "Check out the highest-grossing movies by sorting the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujzmxCnmEclb"
      },
      "source": [
        "# Sort all of the values by gross\n",
        "data['gross_usd'].sort_values(ascending=False).head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VoEEVWDBdO6b"
      },
      "source": [
        "Knowing a bit about movies, it makes sense that the budget of a film might be important to predicting how much money the movie makes, so let's check out the missing budget values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xk_imERleGke"
      },
      "source": [
        "# Identifying the percent of budget values that are missing\n",
        "data['budget_usd'].isnull().mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yp_lHSFzf47g"
      },
      "source": [
        "Since we have between 5-10% of budget values missing, we can impute (or fill in) the missing values without creating too much bias in the data. \n",
        "\n",
        "Using our knowledge of films, I know that film budgets in recent years has increased, so just imputing the mean or median budget for all missing values would not make much sense.  To get around this, we can find the median gross revenue for each year using the Pandas [`groupby`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.groupby.html) function, and then impute based on those values using the [`fillna`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html) function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44aqk0yAtxJ5"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQB4aysxGd04"
      },
      "source": [
        "# Drop movies where the year is missing\n",
        "data.dropna(subset=['title_year'], how='all', inplace=True)\n",
        "# Convert all years to integers\n",
        "data['title_year'] = data['title_year'].astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v2OUoFGFNljS"
      },
      "source": [
        "# Calculate median budgets per year\n",
        "# Impute the median budgets per year for missing budget data\n",
        "data['budget_usd'] = data['budget_usd'].fillna(data.groupby('title_year')['budget_usd'].transform('median'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "US20xbtyJDwK"
      },
      "source": [
        "# Re-check to see if there are still any missing budget values\n",
        "data['budget_usd'].isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnPs2z0tJS3W"
      },
      "source": [
        "# What is the movie with missing budget still?\n",
        "data[data['budget_usd'].isna()].head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTpwgJ5_rUwB"
      },
      "source": [
        "There are no other movies from the year 1942, so we cannot impute based on year.  Other options:\n",
        "- Impute based on the overall median budget\n",
        "- Drop the film from the data\n",
        "- Impute based on other data\n",
        "\n",
        "Since it's only one film, and for the sake of time, we will drop the film from the data set. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "__jRz3D1uGCz"
      },
      "source": [
        "# Drop the row (axis = 0) for the remaining missing value\n",
        "data.***ADD CODE HERE***(subset=['budget_usd'], axis=0, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEc0zl2gK3S3"
      },
      "source": [
        "## Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJMauNTJuFWu"
      },
      "source": [
        "Now, let's take a look at the different countries of origin in the data.  There are likely differences in the gross revenue of movies from different countries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeO2rgWQE1Dd"
      },
      "source": [
        "# Identify all of the unique countries\n",
        "data['country'].***ADD CODE HERE***"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvsxX-5_vMXk"
      },
      "source": [
        "# Get a count for all of the values for each country\n",
        "data['country'].***ADD CODE HERE***"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-FuYixaFf1n"
      },
      "source": [
        "# Find how many movies are in each country\n",
        "counts = data['country'].value_counts()\n",
        "counts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDttBjhgwdA0"
      },
      "source": [
        "Since most movies are from the US, UK, and France let's just focus on movies from those three countries, so the other countries don't skew our predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p1TMNc9vFpHm"
      },
      "source": [
        "# Select just the countries with the 3 largest number of films\n",
        "counts.nlargest(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMiw_ojXF4s3"
      },
      "source": [
        "# Select just the country names of the three largest\n",
        "counts.nlargest(3).***ADD CODE HERE***"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXf9wJbzPFgy"
      },
      "source": [
        "# Select the data from only the top 3 countries\n",
        "data = data[data['country'].isin(counts.nlargest(3).index)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbAd0Khi0aFv"
      },
      "source": [
        "Now, let's take a look at how our cleaning is going."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAEmhrlhyucf"
      },
      "source": [
        "# Check again to see how many missing values we have\n",
        "data.isna().sum().sort_values(ascending = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfxawgh91C-z"
      },
      "source": [
        "We have very few missing values remaining -- if we had more time, we could address each individually, but we don't.  Let's drop all remaining rows that have missing values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npB15jB01bAS"
      },
      "source": [
        "print(data.shape)\n",
        "# Dropping all remaining rows that have null values\n",
        "data.dropna(axis=0, inplace=True)\n",
        "print(data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucVb2yJLDarK"
      },
      "source": [
        "## Feature Engineering\n",
        "Sometimes you might want to use your knowledge of the subject to create new features to help you make predictions\n",
        "\n",
        "Here, I think that the lead actor might have an influence on the gross revenue of a film, so let's check those out to see if we can turn these into a numerical feature for modeling."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpGuP8gDIfmE"
      },
      "source": [
        "# Check out the actors with the most movies in the set\n",
        "data[***ADD CODE HERE***].value_counts().head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gl2sxdVTWH6D"
      },
      "source": [
        "The actors with the most lead roles have 20+ lead roles in this data set.  Let's make a feature that includes whether or not the lead actor has starred in a lot of movies (20+).\n",
        "\n",
        "For this, we will use a list comprehension, which is a concise way to create a list.  The basic syntax for this is:\n",
        "- `[expression if conditional for item in list]`\n",
        "  - where the `expression` is based on an item in the list, \n",
        "  - `if conditional` filters down the list\n",
        "  - `for item in list` breaks down a list into individual items. \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ifdj4adrum1s"
      },
      "source": [
        "# List comprehension example\n",
        "example_list = [1, 2, 3, 4]\n",
        "[number for number in example_list if number % 2 == 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1qVaZorxWCWZ"
      },
      "source": [
        "# Identify all movie counts, select all star actors\n",
        "lead_movie_counts = data['actor_1_name'].value_counts()\n",
        "star_actors = lead_movie_counts[lead_movie_counts>=20].index\n",
        "# Set `lead_star` = 1 if actor is in star_actors, otherwise 0\n",
        "data['lead_star'] = [1 if x in star_actors else 0 for x in data['actor_1_name']]\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMd2zxyC4y8s"
      },
      "source": [
        "Perhaps the `content_rating` will have an impact on the gross revenue of the movie.  We already have this data, but our models will want the information in numerical form, not in words.  \n",
        "\n",
        "To do this, we need to encode the ratings as numbers.  We can use the pandas [`get_dummies`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.get_dummies.html) function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwVDDbpA6dfv"
      },
      "source": [
        "# Encoding ratings as dummy variables\n",
        "content_ratings = pd.***ADD CODE HERE***(data['content_rating'])\n",
        "content_ratings.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LI8MokNW63ql"
      },
      "source": [
        "# Merge the encoded data back on to the original data\n",
        "data = data.join(content_ratings)\n",
        "data.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZ9Ke7i58MMR"
      },
      "source": [
        "Now, we could do a similar type of encoding for any of the other categorical variables, but encoding all of the actor names would create too many columns.  We'll stick with the numerical data for now and select them using the Pandas [`select_dtypes`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.select_dtypes.html) function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KoCvyKTxIayb"
      },
      "source": [
        "# Select columns by data type - number\n",
        "numerical_data = data.***ADD CODE HERE***(include='number')\n",
        "numerical_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0WqRnPfz82dp"
      },
      "source": [
        "Now our `numerical_data` DataFrame is ready for modeling!\n",
        "\n",
        "# Take Home Challenge\n",
        "- Investigate outliers and clean or drop them\n",
        "- Analyze your clean data (see Thinkful's Art of Visualization workshops/webinars for additional support)\n",
        "- Continue to engineer features based on what you know about the subject\n",
        "  - Maybe the era of the film might have something to do with the gross revenue.  The [history of film](http://www.historyoffilm.net/movie-eras/history-of-cinema/) documents the movie eras we can use to create our new feature along with Pandas [`.cut`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.cut.html) function.\n",
        "  - Here is some starter code to get the ball rolling:\n",
        "  - Identify the era of the film\n",
        "\n",
        "    era_bins = [0, 1910, 1926, 1940, 1954, 1976, 2000, 2100]\n",
        "\n",
        "    era_labels = ['pioneer', 'silent', 'talkies', 'golden_era','changes', 'dawn_modern_film', 'modern_film']\n",
        "\n",
        "    data['era'] = pd.cut(...)\n",
        "- Try out different models for predicting the gross revenue of a movie (see Thinkful's Intro to Predictive Modeling workshops/webinars for additional support)\n",
        "- Go back and impute some of the missing data that we dropped"
      ]
    }
  ]
}